{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd Yet-Another-EfficientDet-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "# Author: Zylo117\n",
    "\n",
    "\"\"\"\n",
    "COCO-Style Evaluations\n",
    "\n",
    "put images here datasets/your_project_name/val_set_name/*.jpg\n",
    "put annotations here datasets/your_project_name/annotations/instances_{val_set_name}.json\n",
    "put weights here /path/to/your/weights/*.pth\n",
    "change compound_coef\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess, boolean_string\n",
    "\n",
    "ap = {\n",
    "    'project': 'fashion',\n",
    "    'compound_coef': 0,\n",
    "    'weights': 'logs/efficientdet-d0_0_712 (3).pth',\n",
    "    'nms_threshold': 0.5,\n",
    "    'cuda': False,\n",
    "    'device': 0,\n",
    "    'float16': False,\n",
    "    'override': True\n",
    "}\n",
    "args = argparse.Namespace(**ap)\n",
    "\n",
    "compound_coef = args.compound_coef\n",
    "nms_threshold = args.nms_threshold\n",
    "use_cuda = args.cuda\n",
    "gpu = args.device\n",
    "use_float16 = args.float16\n",
    "override_prev_results = args.override\n",
    "project_name = args.project\n",
    "weights_path = f'weights/efficientdet-d{compound_coef}.pth' if args.weights is None else args.weights\n",
    "\n",
    "print(f'running coco-style evaluation on project {project_name}, weights {weights_path}...')\n",
    "\n",
    "params = yaml.safe_load(open(f'projects/{project_name}.yml'))\n",
    "obj_list = params['obj_list']\n",
    "\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "   \n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name, scores):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id, score in zip(bboxes, category_ids, scores):\n",
    "        if score < 0.3:\n",
    "            continue\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_coco(img_path, set_name, image_ids, coco, model, threshold=0.05):\n",
    "    results = []\n",
    "\n",
    "    regressBoxes = BBoxTransform()\n",
    "    clipBoxes = ClipBoxes()\n",
    "\n",
    "    for image_id in tqdm(image_ids[:5]):\n",
    "        image_info = coco.loadImgs(image_id)[0]\n",
    "        image_path = img_path + image_info['file_name']\n",
    "\n",
    "        ori_imgs, framed_imgs, framed_metas = preprocess(image_path, max_size=input_sizes[compound_coef], mean=params['mean'], std=params['std'])\n",
    "        x = torch.from_numpy(framed_imgs[0])\n",
    "\n",
    "        if use_cuda:\n",
    "            x = x\n",
    "            if use_float16:\n",
    "                x = x.half()\n",
    "            else:\n",
    "                x = x.float()\n",
    "        else:\n",
    "            x = x.float()\n",
    "\n",
    "        x = x.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "        features, regression, classification, anchors = model(x)\n",
    "\n",
    "        preds = postprocess(x,\n",
    "                            anchors, regression, classification,\n",
    "                            regressBoxes, clipBoxes,\n",
    "                            threshold, nms_threshold)\n",
    "        \n",
    "        if not preds:\n",
    "            continue\n",
    "\n",
    "        preds = invert_affine(framed_metas, preds)[0]\n",
    "\n",
    "        scores = preds['scores']\n",
    "        class_ids = preds['class_ids']\n",
    "        rois = preds['rois']\n",
    "\n",
    "        if rois.shape[0] > 0:\n",
    "            # x1,y1,x2,y2 -> x1,y1,w,h\n",
    "            rois[:, 2] -= rois[:, 0]\n",
    "            rois[:, 3] -= rois[:, 1]\n",
    "\n",
    "            bbox_score = scores\n",
    "\n",
    "            for roi_id in range(rois.shape[0]):\n",
    "                score = float(bbox_score[roi_id])\n",
    "                label = int(class_ids[roi_id])\n",
    "                box = rois[roi_id, :]\n",
    "\n",
    "                image_result = {\n",
    "                    'image_id': image_id,\n",
    "                    'category_id': label,\n",
    "                    'score': float(score),\n",
    "                    'bbox': box.tolist(),\n",
    "                }\n",
    "\n",
    "                results.append(image_result)\n",
    "\n",
    "            category_id_to_name = {v: k for v, k in enumerate(obj_list)}\n",
    "            visualize(ori_imgs[0], rois, class_ids, category_id_to_name, scores)\n",
    "            print('plot!')\n",
    "\n",
    "    if not len(results):\n",
    "        raise Exception('the model does not provide any valid output, check model architecture and the data input')\n",
    "\n",
    "    # # write output\n",
    "    # filepath = f'{set_name}_bbox_results.json'\n",
    "    # if os.path.exists(filepath):\n",
    "    #     os.remove(filepath)\n",
    "    # json.dump(results, open(filepath, 'w'), indent=4)\n",
    "\n",
    "\n",
    "def _eval(coco_gt, image_ids, pred_json_path):\n",
    "    # load results in COCO evaluation tool\n",
    "    coco_pred = coco_gt.loadRes(pred_json_path)\n",
    "\n",
    "    # run COCO evaluation\n",
    "    print('BBox')\n",
    "    coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')\n",
    "    coco_eval.params.imgIds = image_ids\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    SET_NAME = params['val_set']\n",
    "    VAL_GT = f'../../{params[\"project_name\"]}/annotations/instances_attributes_{SET_NAME}.json'\n",
    "    VAL_IMGS = f'../../{params[\"project_name\"]}/{SET_NAME}/'\n",
    "    MAX_IMAGES = 10000\n",
    "    coco_gt = COCO(VAL_GT)\n",
    "    image_ids = coco_gt.getImgIds()[:MAX_IMAGES]\n",
    "    \n",
    "    if override_prev_results or not os.path.exists(f'{SET_NAME}_bbox_results.json'):\n",
    "        model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n",
    "                                     ratios=eval(params['anchors_ratios']), scales=eval(params['anchors_scales']))\n",
    "        model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n",
    "        model.requires_grad_(False)\n",
    "        model.eval()\n",
    "\n",
    "        if use_cuda:\n",
    "            model.cuda(gpu)\n",
    "\n",
    "            if use_float16:\n",
    "                model.half()\n",
    "\n",
    "        evaluate_coco(VAL_IMGS, SET_NAME, image_ids, coco_gt, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
